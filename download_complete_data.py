#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´Aè‚¡æ•°æ®æ‰¹é‡ä¸‹è½½è„šæœ¬
ä¸‹è½½æ‰€æœ‰Aè‚¡è‚¡ç¥¨ã€ETFå’ŒæŒ‡æ•°æ•°æ®ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""

import pandas as pd
import akshare as ak
import os
import time
import json
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('complete_data_download.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ•°æ®é…ç½®
DATA_CONFIG = {
    'data_path': './data',
    'start_date': '20200101',  # ä»2020å¹´å¼€å§‹ä¸‹è½½
    'end_date': datetime.now().strftime('%Y%m%d'),
    'max_workers': 6,  # å¹¶å‘çº¿ç¨‹æ•°
    'batch_size': 50   # æ¯æ‰¹ä¸‹è½½æ•°é‡
}

class CompleteDataDownloader:
    """å®Œæ•´æ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self):
        self.data_path = DATA_CONFIG['data_path']
        self.start_date = DATA_CONFIG['start_date']
        self.end_date = DATA_CONFIG['end_date']
        self.max_workers = DATA_CONFIG['max_workers']
        self.batch_size = DATA_CONFIG['batch_size']
        
        # åˆ›å»ºæ•°æ®ç›®å½•
        os.makedirs(self.data_path, exist_ok=True)
        os.makedirs(os.path.join(self.data_path, 'stocks'), exist_ok=True)
        os.makedirs(os.path.join(self.data_path, 'etfs'), exist_ok=True)
        os.makedirs(os.path.join(self.data_path, 'index'), exist_ok=True)
        
    def get_all_stocks(self) -> list:
        """è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨"""
        try:
            stock_list = ak.stock_info_a_code_name()
            stocks = []
            
            for _, row in stock_list.iterrows():
                code = row['code']
                name = row['name']
                exchange = 'SH' if code.startswith(('6', '900', '688')) else 'SZ'
                symbol = f"{code}.{exchange}"
                
                stocks.append({
                    'symbol': symbol,
                    'code': code,
                    'name': name,
                    'exchange': exchange,
                    'type': 'stock'
                })
            
            logger.info(f"è·å–åˆ° {len(stocks)} åªè‚¡ç¥¨")
            return stocks
            
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_all_etfs(self) -> list:
        """è·å–æ‰€æœ‰ETFåˆ—è¡¨"""
        try:
            etf_list = ak.fund_etf_spot_em()
            etfs = []
            
            for _, row in etf_list.iterrows():
                code = row['ä»£ç ']
                name = row['åç§°']
                exchange = 'SH' if code.startswith('51') else 'SZ'
                symbol = f"{code}.{exchange}"
                
                etfs.append({
                    'symbol': symbol,
                    'code': code,
                    'name': name,
                    'exchange': exchange,
                    'type': 'etf'
                })
            
            logger.info(f"è·å–åˆ° {len(etfs)} åªETF")
            return etfs
            
        except Exception as e:
            logger.error(f"è·å–ETFåˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def download_single_symbol(self, symbol_info: dict) -> bool:
        """ä¸‹è½½å•ä¸ªæ ‡çš„çš„å†å²æ•°æ®"""
        symbol = symbol_info['symbol']
        code = symbol_info['code']
        name = symbol_info['name']
        symbol_type = symbol_info['type']
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if symbol_type == 'stock':
                file_path = os.path.join(self.data_path, 'stocks', f"{symbol}.csv")
            else:
                file_path = os.path.join(self.data_path, 'etfs', f"{symbol}.csv")
                
            if os.path.exists(file_path):
                logger.info(f"{symbol} ({name}) æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                return True
            
            # è·å–å†å²æ•°æ®
            if symbol_type == 'stock':
                df = ak.stock_zh_a_hist(
                    symbol=code, 
                    period="daily", 
                    start_date=self.start_date, 
                    end_date=self.end_date, 
                    adjust="qfq"
                )
            else:
                # å°è¯•ETFä¸“ç”¨æ¥å£
                try:
                    df = ak.fund_etf_hist_em(
                        symbol=code, 
                        period="daily", 
                        start_date=self.start_date, 
                        end_date=self.end_date, 
                        adjust="qfq"
                    )
                except:
                    # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨è‚¡ç¥¨æ¥å£
                    df = ak.stock_zh_a_hist(
                        symbol=code, 
                        period="daily", 
                        start_date=self.start_date, 
                        end_date=self.end_date, 
                        adjust="qfq"
                    )
            
            if df.empty:
                logger.warning(f"{symbol} ({name}) æ— æ•°æ®")
                return False
            
            # æ ‡å‡†åŒ–åˆ—å
            if len(df.columns) == 12:
                # ä¸­æ–‡åˆ—åï¼š['æ—¥æœŸ', 'è‚¡ç¥¨ä»£ç ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡']
                df.columns = ['date', 'code', 'open', 'close', 'high', 'low', 'volume', 'amount', 'amplitude', 
                             'change_percent', 'change_amount', 'turnover']
            else:
                # å¤‡ç”¨åˆ—å
                df.columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'amount', 'amplitude', 
                             'change_percent', 'change_amount', 'turnover'][:len(df.columns)]
            
            df['date'] = pd.to_datetime(df['date'])
            df['symbol'] = symbol
            df = df.set_index('date').sort_index()
            
            # ä¿å­˜ä¸ºCSV
            df.to_csv(file_path)
            
            logger.info(f"âœ… æˆåŠŸä¸‹è½½ {symbol} ({name}) - {len(df)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½ {symbol} ({name}) å¤±è´¥: {e}")
            return False
    
    def download_in_batches(self, symbols: list, batch_name: str) -> dict:
        """åˆ†æ‰¹ä¸‹è½½æ•°æ®"""
        total_symbols = len(symbols)
        results = {'success': 0, 'failed': 0}
        
        logger.info(f"å¼€å§‹ä¸‹è½½{batch_name}ï¼Œå…± {total_symbols} ä¸ªæ ‡çš„")
        
        for i in range(0, total_symbols, self.batch_size):
            batch = symbols[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (total_symbols + self.batch_size - 1) // self.batch_size
            
            logger.info(f"ä¸‹è½½æ‰¹æ¬¡ {batch_num}/{total_batches}ï¼Œæœ¬æ‰¹ {len(batch)} ä¸ªæ ‡çš„")
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {executor.submit(self.download_single_symbol, symbol): symbol for symbol in batch}
                
                for future in as_completed(futures):
                    symbol_info = futures[future]
                    try:
                        if future.result():
                            results['success'] += 1
                        else:
                            results['failed'] += 1
                    except Exception as e:
                        logger.error(f"ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}")
                        results['failed'] += 1
            
            # æ‰¹æ¬¡é—´æš‚åœï¼Œé¿å…APIé™åˆ¶
            if i + self.batch_size < total_symbols:
                logger.info("æ‰¹æ¬¡å®Œæˆï¼Œæš‚åœ5ç§’...")
                time.sleep(5)
        
        return results
    
    def download_main_indices(self):
        """ä¸‹è½½ä¸»è¦æŒ‡æ•°æ•°æ®"""
        indices = [
            {'symbol': '000001.SH', 'name': 'ä¸Šè¯æŒ‡æ•°'},
            {'symbol': '000300.SH', 'name': 'æ²ªæ·±300'},
            {'symbol': '000905.SH', 'name': 'ä¸­è¯500'},
            {'symbol': '399001.SZ', 'name': 'æ·±è¯æˆæŒ‡'},
            {'symbol': '399006.SZ', 'name': 'åˆ›ä¸šæ¿æŒ‡'},
            {'symbol': '399005.SZ', 'name': 'ä¸­å°æ¿æŒ‡'},
            {'symbol': '000016.SH', 'name': 'ä¸Šè¯50'},
            {'symbol': '000688.SH', 'name': 'ç§‘åˆ›50'}
        ]
        
        logger.info("å¼€å§‹ä¸‹è½½ä¸»è¦æŒ‡æ•°æ•°æ®...")
        
        for index in indices:
            try:
                symbol = index['symbol']
                name = index['name']
                code = symbol.split('.')[0]
                
                file_path = os.path.join(self.data_path, 'index', f"{symbol}.csv")
                if os.path.exists(file_path):
                    logger.info(f"{symbol} ({name}) æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    continue
                
                df = ak.index_zh_a_hist(
                    symbol=code, 
                    period="daily", 
                    start_date=self.start_date, 
                    end_date=self.end_date
                )
                
                if not df.empty:
                    # æ£€æŸ¥æŒ‡æ•°æ•°æ®çš„å®é™…åˆ—æ•°
                    if len(df.columns) == 11:
                        # ä¸­æ–‡åˆ—åï¼š['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡']
                        df.columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'amount', 'amplitude', 
                                     'change_percent', 'change_amount', 'turnover']
                    elif len(df.columns) == 7:
                        df.columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'amount']
                    else:
                        # å¤‡ç”¨åˆ—å
                        df.columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'amount', 'amplitude', 
                                     'change_percent', 'change_amount', 'turnover'][:len(df.columns)]
                    
                    df['date'] = pd.to_datetime(df['date'])
                    df['symbol'] = symbol
                    df = df.set_index('date').sort_index()
                    
                    df.to_csv(file_path)
                    logger.info(f"âœ… æˆåŠŸä¸‹è½½æŒ‡æ•° {symbol} ({name}) - {len(df)} æ¡è®°å½•")
                
            except Exception as e:
                logger.error(f"âŒ ä¸‹è½½æŒ‡æ•° {symbol} å¤±è´¥: {e}")
    
    def run_download(self):
        """è¿è¡Œæ‰¹é‡ä¸‹è½½"""
        start_time = time.time()
        
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´Aè‚¡æ•°æ®æ‰¹é‡ä¸‹è½½...")
        logger.info(f"æ•°æ®è·¯å¾„: {self.data_path}")
        logger.info(f"æ—¶é—´èŒƒå›´: {self.start_date} è‡³ {self.end_date}")
        
        # è·å–è‚¡ç¥¨å’ŒETFåˆ—è¡¨
        stocks = self.get_all_stocks()
        etfs = self.get_all_etfs()
        
        total_symbols = len(stocks) + len(etfs)
        logger.info(f"æ€»è®¡éœ€è¦ä¸‹è½½: {total_symbols} ä¸ªæ ‡çš„ (è‚¡ç¥¨: {len(stocks)}, ETF: {len(etfs)})")
        
        if total_symbols == 0:
            logger.info("æ— æ•°æ®å¯ä¸‹è½½ï¼")
            return
        
        # åˆ†æ‰¹ä¸‹è½½è‚¡ç¥¨
        if stocks:
            stock_results = self.download_in_batches(stocks, "è‚¡ç¥¨")
        else:
            stock_results = {'success': 0, 'failed': 0}
        
        # åˆ†æ‰¹ä¸‹è½½ETF
        if etfs:
            etf_results = self.download_in_batches(etfs, "ETF")
        else:
            etf_results = {'success': 0, 'failed': 0}
        
        # ä¸‹è½½æŒ‡æ•°æ•°æ®
        self.download_main_indices()
        
        # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
        elapsed_time = time.time() - start_time
        self.generate_report(stock_results, etf_results, elapsed_time)
    
    def generate_report(self, stock_results: dict, etf_results: dict, elapsed_time: float):
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
        report = {
            'download_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'elapsed_time': f"{elapsed_time:.2f}ç§’ ({elapsed_time/60:.2f}åˆ†é’Ÿ)",
            'stocks': {
                'success': stock_results['success'],
                'failed': stock_results['failed'],
                'total': stock_results['success'] + stock_results['failed']
            },
            'etfs': {
                'success': etf_results['success'],
                'failed': etf_results['failed'],
                'total': etf_results['success'] + etf_results['failed']
            },
            'total_downloaded': stock_results['success'] + etf_results['success'],
            'total_failed': stock_results['failed'] + etf_results['failed']
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.data_path, 'complete_download_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æŠ¥å‘Š
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š å®Œæ•´æ•°æ®ä¸‹è½½å®ŒæˆæŠ¥å‘Š")
        logger.info("="*60)
        logger.info(f"ä¸‹è½½æ—¶é—´: {report['download_time']}")
        logger.info(f"è€—æ—¶: {report['elapsed_time']}")
        logger.info(f"è‚¡ç¥¨æˆåŠŸ: {report['stocks']['success']} / {report['stocks']['total']}")
        logger.info(f"ETFæˆåŠŸ: {report['etfs']['success']} / {report['etfs']['total']}")
        logger.info(f"æ€»è®¡æˆåŠŸ: {report['total_downloaded']}")
        logger.info(f"æ€»è®¡å¤±è´¥: {report['total_failed']}")
        logger.info("="*60)
        
        # æ˜¾ç¤ºæ•°æ®ç›®å½•ç»“æ„
        self.show_data_structure()
    
    def show_data_structure(self):
        """æ˜¾ç¤ºæ•°æ®ç›®å½•ç»“æ„"""
        logger.info("\nğŸ“ æ•°æ®æ–‡ä»¶ç»“æ„:")
        
        for data_type in ['stocks', 'etfs', 'index']:
            dir_path = os.path.join(self.data_path, data_type)
            if os.path.exists(dir_path):
                files = [f for f in os.listdir(dir_path) if f.endswith('.csv')]
                logger.info(f"  {data_type}/: {len(files)} ä¸ªCSVæ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    try:
        downloader = CompleteDataDownloader()
        downloader.run_download()
    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­ä¸‹è½½")
    except Exception as e:
        logger.error(f"ä¸‹è½½è¿‡ç¨‹å‡ºç°å¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()