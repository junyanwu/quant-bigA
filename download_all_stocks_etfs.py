#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡ä¸‹è½½Aè‚¡æ‰€æœ‰è‚¡ç¥¨å’ŒETFæ•°æ®å¹¶ä¿å­˜ä¸ºCSVæ–‡ä»¶
ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œä¸‹è½½ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""

import pandas as pd
import akshare as ak
import os
import time
import json
import logging
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_download.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ•°æ®é…ç½®
DATA_CONFIG = {
    'data_path': './data',
    'start_date': '20150101',  # ä»2015å¹´å¼€å§‹ä¸‹è½½
    'end_date': datetime.now().strftime('%Y%m%d'),
    'max_workers': 8,  # å¹¶å‘çº¿ç¨‹æ•°
    'batch_size': 100  # æ¯æ‰¹ä¸‹è½½æ•°é‡
}

class BulkDataDownloader:
    """æ‰¹é‡æ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self):
        self.data_path = DATA_CONFIG['data_path']
        self.start_date = DATA_CONFIG['start_date']
        self.end_date = DATA_CONFIG['end_date']
        self.max_workers = DATA_CONFIG['max_workers']
        self.batch_size = DATA_CONFIG['batch_size']
        
        # åˆ›å»ºæ•°æ®ç›®å½•
        os.makedirs(self.data_path, exist_ok=True)
        os.makedirs(os.path.join(self.data_path, 'stocks'), exist_ok=True)
        os.makedirs(os.path.join(self.data_path, 'etfs'), exist_ok=True)
        os.makedirs(os.path.join(self.data_path, 'index'), exist_ok=True)
        
        # åŠ è½½ä¸‹è½½è¿›åº¦
        self.progress_file = os.path.join(self.data_path, 'download_progress.json')
        self.download_progress = self._load_progress()
        
    def _load_progress(self) -> Dict:
        """åŠ è½½ä¸‹è½½è¿›åº¦"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {'stocks_downloaded': [], 'etfs_downloaded': [], 'failed': []}
    
    def _save_progress(self):
        """ä¿å­˜ä¸‹è½½è¿›åº¦"""
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.download_progress, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def get_all_stocks(self) -> List[Dict]:
        """è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨"""
        try:
            stock_list = ak.stock_info_a_code_name()
            stocks = []
            
            for _, row in stock_list.iterrows():
                code = row['code']
                name = row['name']
                exchange = 'SH' if code.startswith(('6', '900', '688')) else 'SZ'
                symbol = f"{code}.{exchange}"
                
                # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
                if symbol in self.download_progress['stocks_downloaded']:
                    continue
                    
                stocks.append({
                    'symbol': symbol,
                    'code': code,
                    'name': name,
                    'exchange': exchange,
                    'type': 'stock'
                })
            
            logger.info(f"è·å–åˆ° {len(stocks)} åªå¾…ä¸‹è½½è‚¡ç¥¨")
            return stocks
            
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_all_etfs(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ETFåˆ—è¡¨"""
        try:
            etf_list = ak.fund_etf_spot_em()
            etfs = []
            
            for _, row in etf_list.iterrows():
                code = row['ä»£ç ']
                name = row['åç§°']
                exchange = 'SH' if code.startswith('51') else 'SZ'
                symbol = f"{code}.{exchange}"
                
                # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
                if symbol in self.download_progress['etfs_downloaded']:
                    continue
                    
                etfs.append({
                    'symbol': symbol,
                    'code': code,
                    'name': name,
                    'exchange': exchange,
                    'type': 'etf'
                })
            
            logger.info(f"è·å–åˆ° {len(etfs)} åªå¾…ä¸‹è½½ETF")
            return etfs
            
        except Exception as e:
            logger.error(f"è·å–ETFåˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def download_single_symbol(self, symbol_info: Dict) -> bool:
        """ä¸‹è½½å•ä¸ªæ ‡çš„çš„å†å²æ•°æ®"""
        symbol = symbol_info['symbol']
        code = symbol_info['code']
        name = symbol_info['name']
        symbol_type = symbol_info['type']
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if symbol_type == 'stock':
                file_path = os.path.join(self.data_path, 'stocks', f"{symbol}.csv")
            else:
                file_path = os.path.join(self.data_path, 'etfs', f"{symbol}.csv")
                
            if os.path.exists(file_path):
                logger.info(f"{symbol} ({name}) æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                return True
            
            # è·å–å†å²æ•°æ®
            if symbol_type == 'stock':
                df = ak.stock_zh_a_hist(
                    symbol=code, 
                    period="daily", 
                    start_date=self.start_date, 
                    end_date=self.end_date, 
                    adjust="qfq"
                )
            else:
                # å°è¯•ETFä¸“ç”¨æ¥å£
                try:
                    df = ak.fund_etf_hist_em(
                        symbol=code, 
                        period="daily", 
                        start_date=self.start_date, 
                        end_date=self.end_date, 
                        adjust="qfq"
                    )
                except:
                    # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨è‚¡ç¥¨æ¥å£
                    df = ak.stock_zh_a_hist(
                        symbol=code, 
                        period="daily", 
                        start_date=self.start_date, 
                        end_date=self.end_date, 
                        adjust="qfq"
                    )
            
            if df.empty:
                logger.warning(f"{symbol} ({name}) æ— æ•°æ®")
                return False
            
            # æ ‡å‡†åŒ–åˆ—å
            if len(df.columns) >= 11:
                df.columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'amount', 
                             'amplitude', 'change_percent', 'change_amount', 'turnover']
            else:
                # ETFæ•°æ®å¯èƒ½åˆ—æ•°è¾ƒå°‘
                df.columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'amount']
            
            df['date'] = pd.to_datetime(df['date'])
            df['symbol'] = symbol
            df = df.set_index('date').sort_index()
            
            # ä¿å­˜ä¸ºCSV
            df.to_csv(file_path)
            
            # æ›´æ–°è¿›åº¦
            if symbol_type == 'stock':
                self.download_progress['stocks_downloaded'].append(symbol)
            else:
                self.download_progress['etfs_downloaded'].append(symbol)
            
            logger.info(f"âœ… æˆåŠŸä¸‹è½½ {symbol} ({name}) - {len(df)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½ {symbol} ({name}) å¤±è´¥: {e}")
            self.download_progress['failed'].append(symbol)
            return False
    
    def download_in_batches(self, symbols: List[Dict], batch_name: str) -> Dict:
        """åˆ†æ‰¹ä¸‹è½½æ•°æ®"""
        total_symbols = len(symbols)
        results = {'success': 0, 'failed': 0}
        
        logger.info(f"å¼€å§‹ä¸‹è½½{batch_name}ï¼Œå…± {total_symbols} ä¸ªæ ‡çš„")
        
        for i in range(0, total_symbols, self.batch_size):
            batch = symbols[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (total_symbols + self.batch_size - 1) // self.batch_size
            
            logger.info(f"ä¸‹è½½æ‰¹æ¬¡ {batch_num}/{total_batches}ï¼Œæœ¬æ‰¹ {len(batch)} ä¸ªæ ‡çš„")
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {executor.submit(self.download_single_symbol, symbol): symbol for symbol in batch}
                
                for future in as_completed(futures):
                    symbol_info = futures[future]
                    try:
                        if future.result():
                            results['success'] += 1
                        else:
                            results['failed'] += 1
                    except Exception as e:
                        logger.error(f"ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}")
                        results['failed'] += 1
            
            # æ¯æ‰¹å®Œæˆåä¿å­˜è¿›åº¦
            self._save_progress()
            
            # æ‰¹æ¬¡é—´æš‚åœï¼Œé¿å…APIé™åˆ¶
            if i + self.batch_size < total_symbols:
                logger.info("æ‰¹æ¬¡å®Œæˆï¼Œæš‚åœ3ç§’...")
                time.sleep(3)
        
        return results
    
    def download_main_indices(self):
        """ä¸‹è½½ä¸»è¦æŒ‡æ•°æ•°æ®"""
        indices = [
            {'symbol': '000001.SH', 'name': 'ä¸Šè¯æŒ‡æ•°'},
            {'symbol': '000300.SH', 'name': 'æ²ªæ·±300'},
            {'symbol': '000905.SH', 'name': 'ä¸­è¯500'},
            {'symbol': '399001.SZ', 'name': 'æ·±è¯æˆæŒ‡'},
            {'symbol': '399006.SZ', 'name': 'åˆ›ä¸šæ¿æŒ‡'}
        ]
        
        logger.info("å¼€å§‹ä¸‹è½½ä¸»è¦æŒ‡æ•°æ•°æ®...")
        
        for index in indices:
            try:
                symbol = index['symbol']
                name = index['name']
                code = symbol.split('.')[0]
                
                file_path = os.path.join(self.data_path, 'index', f"{symbol}.csv")
                if os.path.exists(file_path):
                    logger.info(f"{symbol} ({name}) æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    continue
                
                df = ak.index_zh_a_hist(
                    symbol=code, 
                    period="daily", 
                    start_date=self.start_date, 
                    end_date=self.end_date
                )
                
                if not df.empty:
                    df.columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'amount']
                    df['date'] = pd.to_datetime(df['date'])
                    df['symbol'] = symbol
                    df = df.set_index('date').sort_index()
                    
                    df.to_csv(file_path)
                    logger.info(f"âœ… æˆåŠŸä¸‹è½½æŒ‡æ•° {symbol} ({name}) - {len(df)} æ¡è®°å½•")
                
            except Exception as e:
                logger.error(f"âŒ ä¸‹è½½æŒ‡æ•° {symbol} å¤±è´¥: {e}")
    
    def run_download(self):
        """è¿è¡Œæ‰¹é‡ä¸‹è½½"""
        start_time = time.time()
        
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½Aè‚¡æ•°æ®...")
        logger.info(f"æ•°æ®è·¯å¾„: {self.data_path}")
        logger.info(f"æ—¶é—´èŒƒå›´: {self.start_date} è‡³ {self.end_date}")
        
        # è·å–è‚¡ç¥¨å’ŒETFåˆ—è¡¨
        stocks = self.get_all_stocks()
        etfs = self.get_all_etfs()
        
        total_symbols = len(stocks) + len(etfs)
        logger.info(f"æ€»è®¡éœ€è¦ä¸‹è½½: {total_symbols} ä¸ªæ ‡çš„ (è‚¡ç¥¨: {len(stocks)}, ETF: {len(etfs)})")
        
        if total_symbols == 0:
            logger.info("æ‰€æœ‰æ•°æ®å·²ä¸‹è½½å®Œæˆï¼")
            return
        
        # åˆ†æ‰¹ä¸‹è½½è‚¡ç¥¨
        if stocks:
            stock_results = self.download_in_batches(stocks, "è‚¡ç¥¨")
        else:
            stock_results = {'success': 0, 'failed': 0}
        
        # åˆ†æ‰¹ä¸‹è½½ETF
        if etfs:
            etf_results = self.download_in_batches(etfs, "ETF")
        else:
            etf_results = {'success': 0, 'failed': 0}
        
        # ä¸‹è½½æŒ‡æ•°æ•°æ®
        self.download_main_indices()
        
        # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
        elapsed_time = time.time() - start_time
        self.generate_report(stock_results, etf_results, elapsed_time)
    
    def generate_report(self, stock_results: Dict, etf_results: Dict, elapsed_time: float):
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
        report = {
            'download_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'elapsed_time': f"{elapsed_time:.2f}ç§’ ({elapsed_time/60:.2f}åˆ†é’Ÿ)",
            'stocks': {
                'success': stock_results['success'],
                'failed': stock_results['failed'],
                'total': stock_results['success'] + stock_results['failed']
            },
            'etfs': {
                'success': etf_results['success'],
                'failed': etf_results['failed'],
                'total': etf_results['success'] + etf_results['failed']
            },
            'total_downloaded': stock_results['success'] + etf_results['success'],
            'total_failed': stock_results['failed'] + etf_results['failed']
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.data_path, 'download_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æŠ¥å‘Š
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š æ•°æ®ä¸‹è½½å®ŒæˆæŠ¥å‘Š")
        logger.info("="*60)
        logger.info(f"ä¸‹è½½æ—¶é—´: {report['download_time']}")
        logger.info(f"è€—æ—¶: {report['elapsed_time']}")
        logger.info(f"è‚¡ç¥¨æˆåŠŸ: {report['stocks']['success']} / {report['stocks']['total']}")
        logger.info(f"ETFæˆåŠŸ: {report['etfs']['success']} / {report['etfs']['total']}")
        logger.info(f"æ€»è®¡æˆåŠŸ: {report['total_downloaded']}")
        logger.info(f"æ€»è®¡å¤±è´¥: {report['total_failed']}")
        logger.info("="*60)
        
        # æ˜¾ç¤ºæ•°æ®ç›®å½•ç»“æ„
        self.show_data_structure()
    
    def show_data_structure(self):
        """æ˜¾ç¤ºæ•°æ®ç›®å½•ç»“æ„"""
        logger.info("\nğŸ“ æ•°æ®æ–‡ä»¶ç»“æ„:")
        
        for data_type in ['stocks', 'etfs', 'index']:
            dir_path = os.path.join(self.data_path, data_type)
            if os.path.exists(dir_path):
                files = [f for f in os.listdir(dir_path) if f.endswith('.csv')]
                logger.info(f"  {data_type}/: {len(files)} ä¸ªCSVæ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    try:
        downloader = BulkDataDownloader()
        downloader.run_download()
    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­ä¸‹è½½ï¼Œè¿›åº¦å·²ä¿å­˜")
    except Exception as e:
        logger.error(f"ä¸‹è½½è¿‡ç¨‹å‡ºç°å¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()